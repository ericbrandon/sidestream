//! LLM orchestration module
//!
//! This module handles routing chat requests to the appropriate provider
//! (Anthropic, OpenAI, or Gemini) and manages stream state.
//!
//! Provider-specific implementations are in separate modules:
//! - `llm_anthropic` - Anthropic Claude API
//! - `llm_openai` - OpenAI Responses API
//! - `llm_gemini` - Google Gemini API
//! - `llm_voice` - Voice message handling (Gemini-based)

use std::sync::Arc;

use serde::{Deserialize, Serialize};
use tokio::sync::Mutex;
use tokio_util::sync::CancellationToken;

use crate::llm_anthropic::send_chat_message_anthropic;
use crate::llm_gemini::send_chat_message_gemini;
use crate::llm_openai::send_chat_message_openai;
use crate::llm_voice::{send_voice_message_impl, transcribe_audio_gemini_impl};
use crate::providers::anthropic::{Citation, InlineCitation};

/// Shared state for managing stream cancellation
pub struct StreamState {
    pub cancel_token: Arc<Mutex<Option<CancellationToken>>>,
}

#[tauri::command]
pub async fn cancel_chat_stream(state: tauri::State<'_, StreamState>) -> Result<(), String> {
    let mut token_guard = state.cancel_token.lock().await;
    if let Some(token) = token_guard.take() {
        token.cancel();
    }
    Ok(())
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ChatMessage {
    pub role: String,
    pub content: serde_json::Value,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct StreamDelta {
    pub turn_id: String,
    pub text: String,
    pub citations: Option<Vec<Citation>>,
    pub inline_citations: Option<Vec<InlineCitation>>,
    pub thinking: Option<String>,
    pub execution: Option<ExecutionDelta>,
}

/// Delta for code execution events (Claude code_execution, OpenAI code_interpreter)
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ExecutionDelta {
    /// Tool name: "bash_code_execution", "text_editor_code_execution", "code_interpreter"
    pub tool_name: String,
    /// Standard output from code execution
    pub stdout: Option<String>,
    /// Standard error from code execution
    pub stderr: Option<String>,
    /// Current execution status
    pub status: ExecutionStatus,
    /// The code being executed (sent at start)
    pub code: Option<String>,
    /// Files generated by the execution (sent on completion)
    pub files: Option<Vec<GeneratedFile>>,
}

/// Status of code execution
#[derive(Debug, Serialize, Deserialize, Clone, PartialEq)]
#[serde(rename_all = "snake_case")]
pub enum ExecutionStatus {
    /// Execution has started
    Started,
    /// Execution is in progress
    Running,
    /// Execution completed successfully
    Completed,
    /// Execution failed with an error
    Failed { error: String },
}

/// A file generated by code execution
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct GeneratedFile {
    /// Provider-specific file ID for downloading
    pub file_id: String,
    /// Original filename
    pub filename: String,
    /// MIME type of the file
    pub mime_type: Option<String>,
}

/// Event payload for stream completion/cancellation events
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct StreamEvent {
    pub turn_id: String,
}

/// Event payload for container ID updates (Claude code execution)
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ContainerIdEvent {
    pub turn_id: String,
    pub container_id: String,
}

/// Determine which provider to use based on model name
fn get_provider_for_model(model: &str) -> &'static str {
    if model.starts_with("gpt") || model.starts_with("o3") || model.starts_with("o4") {
        "openai"
    } else if model.starts_with("gemini") {
        "google"
    } else {
        "anthropic" // Default for claude-* and unknown models
    }
}

#[tauri::command]
pub async fn send_chat_message(
    app: tauri::AppHandle,
    window: tauri::Window,
    state: tauri::State<'_, StreamState>,
    model: String,
    messages: Vec<ChatMessage>,
    system_prompt: Option<String>,
    extended_thinking_enabled: bool,
    thinking_budget: Option<u32>,
    web_search_enabled: bool,
    code_execution_enabled: bool,           // For Anthropic/OpenAI code execution
    reasoning_level: Option<String>,        // For OpenAI: "off", "low", "medium", "high"
    gemini_thinking_level: Option<String>,  // For Gemini: "off", "on", "low", "medium", "high"
    session_id: Option<String>,             // For OpenAI prompt caching
    turn_id: String,                        // Unique ID for this conversation turn
    anthropic_container_id: Option<String>, // Claude code execution container ID for sandbox persistence
) -> Result<(), String> {
    // Create a cancellation token for this stream
    let cancel_token = CancellationToken::new();
    {
        let mut token_guard = state.cancel_token.lock().await;
        *token_guard = Some(cancel_token.clone());
    }

    // Route to the appropriate provider based on model
    let provider = get_provider_for_model(&model);

    match provider {
        "openai" => {
            send_chat_message_openai(
                &app,
                &window,
                cancel_token,
                model,
                messages,
                system_prompt,
                web_search_enabled,
                reasoning_level,
                session_id,
                turn_id,
            )
            .await
        }
        "google" => {
            send_chat_message_gemini(
                &app,
                &window,
                cancel_token,
                model,
                messages,
                system_prompt,
                web_search_enabled,
                gemini_thinking_level,
                turn_id,
            )
            .await
        }
        "anthropic" | _ => {
            send_chat_message_anthropic(
                &app,
                &window,
                cancel_token,
                model,
                messages,
                system_prompt,
                extended_thinking_enabled,
                thinking_budget,
                web_search_enabled,
                code_execution_enabled,
                turn_id,
                anthropic_container_id,
            )
            .await
        }
    }
}

/// Send a voice message with native audio to Gemini
/// The audio is sent as inlineData, and Gemini's response includes the transcription
/// wrapped in [TRANSCRIPTION][/TRANSCRIPTION] tags, followed by the actual response.
#[tauri::command]
pub async fn send_voice_message(
    app: tauri::AppHandle,
    window: tauri::Window,
    state: tauri::State<'_, StreamState>,
    model: String,
    messages: Vec<ChatMessage>,
    audio_base64: String,
    system_prompt: Option<String>,
    web_search_enabled: bool,
    gemini_thinking_level: Option<String>,
    turn_id: String,
) -> Result<(), String> {
    // Create a cancellation token for this stream
    let cancel_token = CancellationToken::new();
    {
        let mut token_guard = state.cancel_token.lock().await;
        *token_guard = Some(cancel_token.clone());
    }

    send_voice_message_impl(
        &app,
        &window,
        cancel_token,
        model,
        messages,
        audio_base64,
        system_prompt,
        web_search_enabled,
        gemini_thinking_level,
        turn_id,
    )
    .await
}

/// Transcribe audio using Gemini (transcription only, no chat response)
#[tauri::command]
pub async fn transcribe_audio_gemini(
    app: tauri::AppHandle,
    audio_base64: String,
) -> Result<String, String> {
    transcribe_audio_gemini_impl(&app, audio_base64).await
}
